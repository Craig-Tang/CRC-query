{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reliable as rc\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import joblib\n",
    "import json as js\n",
    "import networkx as nx\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from csv import reader\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/'\n",
    "\n",
    "Config = {\n",
    "    'StackOverFlow':{'k': 27}, \n",
    "    'HepPh':{'k': 118},\n",
    "    'Reddit':{'k': 12}, \n",
    "    'Email_EU':{'k': 8}, \n",
    "}\n",
    "\n",
    "data_pick = ['Email_EU', 'Reddit', 'HepPh', 'StackOverFlow']\n",
    "query_all = joblib.load('input/query_all.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(data_pick):\n",
    "    graph_path = data_path + data + '/'\n",
    "    graphs = sorted(os.listdir(graph_path))\n",
    "    print(graphs)\n",
    "    list_G_ori = Parallel(n_jobs=-1, verbose=0)(delayed(nx.read_gml)(graph_path+g) for g in graphs[:20])\n",
    "\n",
    "    start_time0 = time.process_time()\n",
    "    # construction\n",
    "    theta_thres_all = []\n",
    "    wcf_indices = []\n",
    "\n",
    "    print('Constructing Index for: {} with {} instances.'.format(data, len(list_G_ori)))\n",
    "    theta_thres_all = Parallel(n_jobs=-1, verbose=1)(delayed(rc.theta_thres_table)(g) for g in list_G_ori)\n",
    "    wcf_indices = Parallel(n_jobs=-1, verbose=1)(delayed(rc.theta_tree)(theta_thres_all[i],g) for i,g in enumerate(list_G_ori))\n",
    "    joblib.dump(wcf_indices, 'input/wcf_indices-'+data+'.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying k\n",
    "\n",
    "theta = 0.4\n",
    "k_all = [0.2,0.4,0.6,0.8]\n",
    "data_pick = ['Email_EU', 'Reddit', 'HepPh', 'StackOverFlow']\n",
    "data_pick.reverse()\n",
    "iterables = [data_pick, k_all]\n",
    "index = pd.MultiIndex.from_product(iterables, names=['data', 'k_ratio'])\n",
    "\n",
    "vary_k = pd.DataFrame(columns=['EEF', 'WCF'], index=index)\n",
    "\n",
    "for i, data in enumerate(data_pick):\n",
    "    print('For dataset {}'.format(data))\n",
    "    graph_path = data_path + data + '/'\n",
    "    graphs = os.listdir(graph_path)\n",
    "    list_G_ori = Parallel(n_jobs=-1, verbose=10)(delayed(nx.read_gml)(graph_path+g) for g in graphs[:10])\n",
    "\n",
    "    # construction\n",
    "    wcf_indices = joblib.load('input/wcf_indices-'+data+'.pkl.zip')[:10]\n",
    "\n",
    "    EB_time = []\n",
    "    IB_time = []\n",
    "    for k_ratio in tqdm(k_all, desc='Processing query K'):\n",
    "        k = int(Config[data]['k']*k_ratio)\n",
    "        list_G = list_G_ori.copy()\n",
    "        for query in tqdm(query_all[data]):\n",
    "            \n",
    "            start_time1 = time.time()\n",
    "            g1, score1 = rc.EEF(list_G, query, theta, k)\n",
    "            end_time1 = time.time ()\n",
    "            EB_time.append(end_time1 - start_time1)\n",
    "\n",
    "            start_time2 = time.time()\n",
    "            g2, score2 = rc.WCF_search(list_G, wcf_indices, query, theta, k)\n",
    "            end_time2 = time.time()\n",
    "            IB_time.append(end_time2 - start_time2)\n",
    "\n",
    "        vary_k.loc[(data, k_ratio)] = {'EEF': mean(EB_time), 'WCF': mean(IB_time)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying theta\n",
    "\n",
    "k_ratio = 0.4\n",
    "theta_all = [0.0,0.2,0.4,0.6,0.8]\n",
    "data_pick = ['Email_EU', 'Reddit', 'HepPh', 'StackOverFlow']\n",
    "iterables = [data_pick, theta_all]\n",
    "index = pd.MultiIndex.from_product(iterables, names=['data', 'theta'])\n",
    "\n",
    "vary_theta = pd.DataFrame(columns=['EEF', 'WCF'], index=index)\n",
    "\n",
    "for i, data in enumerate(data_pick):\n",
    "    print('For dataset {}'.format(data))\n",
    "    graph_path = data_path + data + '/'\n",
    "    graphs = os.listdir(graph_path)\n",
    "    list_G_ori = Parallel(n_jobs=-1)(delayed(nx.read_gml)(graph_path+g) for g in graphs[:10])\n",
    "\n",
    "    # construction\n",
    "    wcf_indices = joblib.load('input/wcf_indices-'+data+'.pkl.zip')[:10]\n",
    "\n",
    "    EB_time = []\n",
    "    IB_time = []\n",
    "    for theta in tqdm(theta_all, desc='Processing query theta'):\n",
    "        k = int(Config[data]['k']*k_ratio)\n",
    "        list_G = list_G_ori.copy()\n",
    "        for query in tqdm(query_all[data]):\n",
    "            \n",
    "            # print('\\t\\tProcessing query K={}'.format(k))\n",
    "            start_time1 = time.time()\n",
    "            g1, score1 = rc.EEF(list_G, query, theta, k)\n",
    "            end_time1 = time.time ()\n",
    "            EB_time.append(end_time1 - start_time1)\n",
    "\n",
    "            start_time2 = time.time()\n",
    "            g2, score2 = rc.WCF_search(list_G, wcf_indices, query, theta, k)\n",
    "            end_time2 = time.time()\n",
    "            IB_time.append(end_time2 - start_time2)\n",
    "\n",
    "        vary_theta.loc[(data, theta)] = {'EEF': mean(EB_time), 'WCF': mean(IB_time)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying T\n",
    "\n",
    "theta = 0.4\n",
    "t_all = [2,5,10,15,20]\n",
    "data_pick = ['Email_EU', 'Reddit', 'HepPh', 'StackOverFlow']\n",
    "iterables = [data_pick, t_all]\n",
    "index = pd.MultiIndex.from_product(iterables, names=['data', 't'])\n",
    "\n",
    "vary_t = pd.DataFrame(columns=['EEF', 'WCF'], index=index)\n",
    "\n",
    "for i, data in enumerate(data_pick):\n",
    "    print('For dataset {}'.format(data))\n",
    "    graph_path = data_path + data + '/'\n",
    "    graphs = os.listdir(graph_path)\n",
    "    list_G_ori = Parallel(n_jobs=-1)(delayed(nx.read_gml)(graph_path+g) for g in graphs[:20])\n",
    "\n",
    "    # construction\n",
    "    wcf_indices_ori = joblib.load('input/wcf_indices-'+data+'.pkl.zip')[:20]\n",
    "\n",
    "    EB_time = []\n",
    "    IB_time = []\n",
    "    for t in tqdm(t_all, desc='Processing query t'):\n",
    "        k = int(Config[data]['k']*0.4)\n",
    "        list_G = list_G_ori[:t].copy()\n",
    "        wcf_indices = wcf_indices_ori[:t].copy()\n",
    "        for query in tqdm(query_all[data]):\n",
    "            \n",
    "            start_time1 = time.time()\n",
    "            g1, score1 = rc.EEF(list_G, query, theta, k)\n",
    "            end_time1 = time.time ()\n",
    "            EB_time.append(end_time1 - start_time1)\n",
    "\n",
    "            start_time2 = time.time()\n",
    "            g2, score2 = rc.WCF_search(list_G, wcf_indices, query, theta, k)\n",
    "            end_time2 = time.time()\n",
    "            IB_time.append(end_time2 - start_time2)\n",
    "\n",
    "        vary_t.loc[(data, t)] = {'EEF': mean(EB_time), 'WCF': mean(IB_time)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'Reddit'\n",
    "graph_path = data_path + data + '/'\n",
    "graph_test = os.listdir(graph_path)[0]\n",
    "g_test = nx.read_gml(graph_path+graph_test)\n",
    "theta_thres_df_old = rc.theta_thres_table(g_test)\n",
    "wcf_index = rc.theta_tree(theta_thres_df_old,g_test)\n",
    "\n",
    "# ratios = [0.01, 0.03, 0.05]\n",
    "ratios = [100, 200, 300, 500, 1000]\n",
    "recons_time = pd.DataFrame(columns=['Reconstruct','Maintenance'], index=ratios)\n",
    "for ratio in ratios:\n",
    "    # sampled_edges = random.sample(g_test.edges, int(ratio*len(g_test.edges)))\n",
    "    sampled_edges = random.sample(g_test.edges, ratio)\n",
    "    g_test_new = g_test.copy()\n",
    "\n",
    "    update_v_list = set()\n",
    "    for e in tqdm(sampled_edges):\n",
    "        new_weight = random.randint(0,10)*0.1\n",
    "        g_test_new.add_edge(*e, weight=new_weight)\n",
    "\n",
    "        # update from removing\n",
    "        theta1 = g_test[e[0]][e[1]]['weight']\n",
    "        subcore = []\n",
    "        subcore.extend(list(rc.sub_core(g_test,e[0])[1].keys()))\n",
    "        subcore.extend(list(rc.sub_core(g_test,e[1])[1].keys()))\n",
    "        subcore = [i for i in subcore if (theta_thres_df_old.loc[i]<=theta1).any()>0]\n",
    "        update_v_list.update(subcore)\n",
    "\n",
    "        # update from inserting\n",
    "        theta2 = new_weight\n",
    "        purecore = []\n",
    "        purecore.extend(list(rc.pure_core(g_test,e[0])[1].keys()))\n",
    "        purecore.extend(list(rc.pure_core(g_test,e[1])[1].keys()))\n",
    "        purecore = [i for i in purecore if (theta_thres_df_old.loc[i]<=theta2).any()>0]\n",
    "        update_v_list.update(purecore)\n",
    "    print(len(update_v_list))\n",
    "\n",
    "    ts = time.time()\n",
    "    tt0 = rc.theta_thres_table(g_test_new)\n",
    "    new_wcf_index0 = rc.theta_tree(tt0, g_test_new)\n",
    "    t0 = time.time()-ts\n",
    "\n",
    "    ts = time.time()\n",
    "    new_wcf_index1 = rc.maintenance_index(list(update_v_list),g_test_new,new_wcf_index0, theta_thres_df_old)\n",
    "    t1 = time.time()-ts\n",
    "    recons_time.loc[ratio] = {'Reconstruct':t0, 'Maintenance':t1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '...'\n",
    "for data in tqdm(data_pick):\n",
    "    graph_path = data_path + data + '/'\n",
    "    graphs = os.listdir(graph_path)\n",
    "    list_G_ori = Parallel(n_jobs=-1)(delayed(nx.read_gml)(graph_path+g) for g in graphs)\n",
    "    # construction\n",
    "    theta_thres_all = []\n",
    "    wcf_indices = []\n",
    "\n",
    "    theta_thres_all = Parallel(n_jobs=-1, verbose=10)(delayed(rc.theta_thres_table)(g) for g in list_G_ori)\n",
    "    wcf_indices = Parallel(n_jobs=-1, verbose=10)(delayed(rc.theta_tree)(theta_thres_all[i],g) for i,g in enumerate(list_G_ori))\n",
    "    \n",
    "    # joblib.dump(wcf_indices, save_path + data+'_indices.pkl.zip')\n",
    "\n",
    "    old_indices = rc.indices_to_json(wcf_indices)\n",
    "    with open(save_path + data+'_old_indices.json', 'w') as fp:\n",
    "        js.dump(old_indices, fp)\n",
    "\n",
    "    compressed_index, auxiliary_table = rc.compress_wcf_indices(wcf_indices, 1)\n",
    "    new_indices = rc.indices_to_json(compressed_index)\n",
    "    with open(save_path + data+'_new_indices.json', 'w') as fp:\n",
    "        js.dump(new_indices, fp)\n",
    "\n",
    "    with open(save_path + data+'_auxiliary_table.json', 'w') as fp:\n",
    "        js.dump(auxiliary_table, fp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
